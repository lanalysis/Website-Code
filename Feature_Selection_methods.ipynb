{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the mutual information regression that uses KNN - uses preassigned number of features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from functools import partial\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    " \n",
    "score_func = partial(mutual_info_regression, discrete_features=[0], random_state=68)\n",
    " \n",
    "selection = SelectKBest(score_func=score_func, k=10)\n",
    "#more features more better\n",
    " \n",
    "selection.fit_transform(X, y)\n",
    "\n",
    "print(X[X.columns[selection.get_support(indices=True)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60982863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses a random forest regression in order to determine importances for each category by using gini coefficient\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "modelRF = RandomForestRegressor(n_estimators=100)\n",
    "modelRF.fit(X,y)\n",
    "importances = modelRF.feature_importances_\n",
    "\n",
    "\n",
    "finaldf = pd.DataFrame({\"Features\":pd.DataFrame(X).columns,\"Importances\":importances})\n",
    "finaldf.set_index(\"Importances\")\n",
    "finaldf = finaldf.sort_values(\"Importances\")\n",
    "finaldf.plot.bar(color = \"teal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd29f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses an exhaustive feature selection to test all possible combinations of the features (it takes a long time) \n",
    "\n",
    "#number of combinations is summation from min features to max features of (maxfeaturesCn)\n",
    "\n",
    "RF = RandomForestRegressor()\n",
    "\n",
    "efs = EFS(RF, min_features = 1, max_features = 15, scoring = \"neg_mean_squared_error\", print_progress = True,cv=5,n_jobs=-1)\n",
    "\n",
    "efs.fit(X,y)\n",
    "\n",
    "print(\"Best MSE Score: %.2f\"% efs.best_score_ *(-1))\n",
    "print(\"Best subset:\", efs.best_idx_)\n",
    "selected_features = X.columns[list(efs.best_idx_)]\n",
    "print(selected_features)\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential feature selection using Support vector regression - uses preassigned number of features\n",
    "\n",
    "#SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df.loc[:,'calib':'pLowEffortLowRewardPressRate']\n",
    "\n",
    "regress = SVR(kernel = 'rbf', gamma = 0.05)\n",
    "sffsf = SFS(regress, k_features=8, forward=True,floating=False, verbose=0,scoring=None, cv=2, n_jobs=1,).fit(X, y)\n",
    "print(sffsf.k_feature_names_)\n",
    "\n",
    "\n",
    "plot_sfs(sffsf.get_metric_dict())\n",
    "plt.show()\n",
    "\n",
    "#sequential feature selection using random forest regressor \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regress = RandomForestRegressor(n_estimators = 30, random_state = 0)\n",
    "X = df.loc[:,'calib':'pLowEffortLowRewardPressRate']\n",
    "sffsf = SFS(regress, k_features=50, forward=True,floating=False, verbose=0,scoring=None, cv=2, n_jobs=1,).fit(X, y)\n",
    "print(sffsf.k_feature_names_)\n",
    "\n",
    "plot_sfs(sffsf.get_metric_dict())\n",
    "plt.show()\n",
    "\n",
    "#sequential feature selection using a sklearn ANN\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sympy.combinatorics.subsets import Subset\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:,'calib':'pLowEffortLowRewardPressRate']\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(2000,5), activation='relu', solver='adam', max_iter=5000)\n",
    "\n",
    "sffsf = SFS(clf, k_features=5, forward=True,floating=True, verbose=0,scoring=None, cv=2, n_jobs=1,).fit(X, y)\n",
    "print(sffsf.k_feature_names_)\n",
    "\n",
    "plot_sfs(sffsf.get_metric_dict())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#SFS can be used with any machine learning method (logistic regression, SVR, ANN, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutation test\n",
    "\n",
    "\n",
    "p = 50000\n",
    "permute_df = pd.DataFrame(columns=['permuted_avg_mse'])\n",
    "\n",
    "#names dataframe\n",
    "\n",
    "for i in range (p):\n",
    "    df1['rateExerciseCompletion'] = df1['rateExerciseCompletion'].sample(n=19).values\n",
    "    X = df1.loc[:,['ctRmHighEffortHighReward','ctRmMedEffort','ctRmLowEffortMedReward','LowEffortLowRewardPts','ctRmMedReward','ctRmMedEffortMedReward','HighEffortLowRewardTime']]\n",
    "    y = df1.loc[:,['rateExerciseCompletion']]\n",
    "    loss1, avg_mse1 = model.evaluate(X,y, batch_size=16,verbose=1)\n",
    "    permute_df.loc[i, 'permuted_avg_mse'] = avg_mse1\n",
    "    \n",
    "    \n",
    "#evaluates mse for P permutations using the model and saves them to dataframe \n",
    "\n",
    "count = 0\n",
    "for i in range (p):\n",
    "    if permute_df.iloc[i,0]<=avg_mse:\n",
    "        count+=1\n",
    "p_value = ((count+1)/(p+1))\n",
    "print(\"P_value for overall average MSE:\", p_value)\n",
    "print(\"Count for overall average MSE:\",count)\n",
    "\n",
    "\n",
    "\n",
    "#count is number of times permutation mse is lower than avg_mse for model, p_value is ratio of count+1 over P+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
